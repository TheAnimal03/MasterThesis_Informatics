{"cells":[{"cell_type":"markdown","metadata":{"id":"SZy7hBHc71LY"},"source":["# Import Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"J8yDd36WBBdy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669050075453,"user_tz":-60,"elapsed":4625,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"}},"outputId":"e8ee1a76-3abd-4c61-d1bd-23dfeb8c27f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab started\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/MyDrive/Uni/MA  exists.\n","\n","Colab: Changing directory to  /content/gdrive/MyDrive/Uni/MA\n","/content/gdrive/MyDrive/Uni/MA\n"]}],"source":["# Header\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Colab started\")\n","  mount=\"/content/gdrive\" #/content/gdrive/MyDrive/Uni/MA\n","  print(\"Colab: mounting Google drive on \", mount)\n","\n","  drive.mount(mount, force_remount=True)\n","    # Switch to the directory on the Google Drive that you want to use\n","  import os\n","  myroot = \"/MyDrive/Uni/MA\"\n","  drive_root = mount +  myroot #+\" Notebooks/NYU/demo\"\n","\n","    # Create drive_root if it doesn't exist\n","  create_drive_root = True\n","  if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","  \n","  # Change to the directory\n","  print(\"\\nColab: Changing directory to \", drive_root)\n","  %cd $drive_root"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o08-TCFLNj3F"},"outputs":[],"source":["#!chmod +x ./darknet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugtO52QvcZre"},"outputs":[],"source":["#!python train.py --help"]},{"cell_type":"markdown","metadata":{"id":"nJuEowwbaSXQ"},"source":[]},{"cell_type":"markdown","metadata":{"id":"wxc2etHpJQl3"},"source":["# YOLOv4 Darknet"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1669050087164,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"},"user_tz":-60},"id":"CEkZevUB1GfM","outputId":"34a2ab28-60a5-43b5-b56e-8044d315e2cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archiv\t\t\t       pytorch-YOLOv4\n","coco\t\t\t       RKD.docx\n","MasterThesis1.0.pdf\t       RKD.pdf\n","MasterThesis_Informatics       SimpleNN.pdf\n","MA_Zusammenfassung_DeepL.docx  train2017.txt\n","MA_Zusammenfassung_DeepL.pdf   Vollstaendige_Aufgabenstellung_MA_Tchenko.pdf\n","My_MA_Schedule.xlsx\t       yolov4_mulltibackbones\n","PC_Schedule.xlsx\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":911,"status":"ok","timestamp":1669050098006,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"},"user_tz":-60},"id":"HrM_1jjgy_oA","outputId":"b63720e9-d074-498a-efd3-3143557138a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4\n"]}],"source":["#%cd Darknet_YOLOv4/\n","%cd pytorch-YOLOv4"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669050102027,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"},"user_tz":-60},"id":"PeSLPZ4kzSv5","outputId":"e70b2ec0-3d36-4133-f530-d509923da9da"},"outputs":[{"output_type":"stream","name":"stdout","text":["cfg\t\t      demo_tensorflow.py   __pycache__\n","cfg_Lab.py\t      demo_trt.py\t   README.md\n","cfgmnv2.py\t      evaluate_on_coco.py  readme.txt\n","cfg.py\t\t      INTERRUPTED.pth\t   requirements.txt\n","checkpoints\t      License.txt\t   tool\n","data\t\t      log\t\t   train2017.txt\n","dataset.py\t      modelsmnv2.py\t   trainmnv2.py\n","DeepStream\t      models.py\t\t   train.py\n","demo_darknet2onnx.py  playground.ipynb\t   trainresult.txt\n","demo.py\t\t      playground.py\t   Use_yolov4_to_train_your_own_data.md\n","demo_pytorch2onnx.py  predictions.jpg\t   val2017.txt\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3542,"status":"ok","timestamp":1669050111968,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"},"user_tz":-60},"id":"KE4xiBTCEVj5","outputId":"591e144c-1ad1-4866-dc6a-c36e080533f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.19.6)\n"]}],"source":["#!python train.py --device 0 --batch-size 16 --img 640 640 --data coco.yaml --cfg cfg/yolov4-pacsp.cfg --weights weights/yolov4.weights --name yolov4-pacsp\n","\n","!pip install tensorboardX"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zI00SLhWUJSL","outputId":"88d97c78-2893-4126-a777-8d088e6585fe","executionInfo":{"status":"ok","timestamp":1669051882152,"user_tz":-60,"elapsed":154579,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["log file path:log/log_2022-11-21_17-28-51.txt\n","Test:  False\n","device:  cpu\n","2022-11-21 17:28:51,772 train.py[line:674] INFO: Using device cpu\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","2022-11-21 17:28:52,525 train.py[line:334] INFO: Starting training:\n","        Epochs:          320\n","        Batch size:      64\n","        Subdivisions:    16\n","        Learning rate:   0.001\n","        Training size:   10\n","        Validation size: 4\n","        Checkpoints:     True\n","        Device:          cpu\n","        Images size:     608\n","        Optimizer:       adam\n","        Dataset classes: 80\n","        Train label path:/content/gdrive/MyDrive/Uni/MA/coco/train2017_.txt\n","        Pretrained:\n","    \n","Epoch 1/320:  80%|▊| 8/10 [02:03<00:30, 15.45s/imgin function convert_to_coco_api...\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","creating index...\n","index created!\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","-------------------- (22743, 1, 2)\n","Accumulating evaluation results...\n","DONE (t=0.02s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n","2022-11-21 17:31:17,136 train.py[line:484] INFO: Created checkpoint directory\n","2022-11-21 17:31:18,305 train.py[line:500] INFO: Checkpoint 1 saved !\n","Epoch 1/320:  80%|▊| 8/10 [02:25<00:36, 18.22s/img\n","Epoch 2/320:   0%|        | 0/10 [00:03<?, ?img/s]\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n","    send_bytes(obj)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","2022-11-21 17:31:23,355 train.py[line:695] INFO: Saved interrupt\n"]}],"source":["# Train\n","!python train.py -g 0 -dir '/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/data/train2017/'"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJBhqbUMiB-9","executionInfo":{"status":"ok","timestamp":1669052926363,"user_tz":-60,"elapsed":117798,"user":{"displayName":"Christian Tchenko","userId":"06607555822736271540"}},"outputId":"c296b299-1ee7-4e8d-cc30-b5503ed36e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["log file path:log/log_2022-11-21_17-46-52.txt\n","Test:  False\n","device:  cpu\n","2022-11-21 17:46:52,551 trainmnv2.py[line:688] INFO: Using device cpu\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","2022-11-21 17:46:53,269 trainmnv2.py[line:338] INFO: Starting training:\n","        Epochs:          1000\n","        Batch size:      64\n","        Subdivisions:    16\n","        Learning rate:   0.001\n","        Training size:   10\n","        Validation size: 4\n","        Checkpoints:     True\n","        Device:          cpu\n","        Images size:     608\n","        Optimizer:       adam\n","        Dataset classes: 80\n","        Train label path:/content/gdrive/MyDrive/Uni/MA/coco/train2017_.txt\n","        Pretrained:\n","    \n","Epoch 1/1000:   0%|       | 0/10 [00:00<?, ?img/s]Batchsize Test:  torch.Size([4, 255, 38, 38])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 76, 76])\n","Anchor:  torch.Size([4, 3, 85, 76, 76])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 38, 38])\n","Anchor:  torch.Size([4, 3, 85, 38, 38])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 19, 19])\n","Anchor:  torch.Size([4, 3, 85, 19, 19])\n","Epoch 1/1000:  40%|▍| 4/10 [00:57<01:25, 14.28s/imBatchsize Test:  torch.Size([4, 255, 38, 38])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 76, 76])\n","Anchor:  torch.Size([4, 3, 85, 76, 76])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 38, 38])\n","Anchor:  torch.Size([4, 3, 85, 38, 38])\n","Test size:  4\n","Anchor:  torch.Size([4, 255, 19, 19])\n","Anchor:  torch.Size([4, 3, 85, 19, 19])\n","Epoch 1/1000:  80%|▊| 8/10 [01:50<00:27, 13.66s/im******************** Initing MobilenetV2 weights ********************\n","initing Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n","initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","initing Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","**************************************** \n","Loading weight of MobilenetV2 : /content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/cfg/mobilenetv2-c5e733a8.pth\n","Loaded weight of MobilenetV2 : /content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/cfg/mobilenetv2-c5e733a8.pth\n","in function convert_to_coco_api...\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","creating index...\n","index created!\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","You could also create your own 'get_image_id' function.\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","-------------------- (32, 76, 74)\n","Epoch 1/1000:  80%|▊| 8/10 [01:53<00:28, 14.20s/im\n","Traceback (most recent call last):\n","  File \"trainmnv2.py\", line 710, in <module>\n","    device=device, )\n","  File \"trainmnv2.py\", line 462, in train\n","    evaluator = evaluate(eval_model, val_loader, config, device)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"trainmnv2.py\", line 585, in evaluate\n","    coco_evaluator.update(res)\n","  File \"/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/tool/tv_reference/coco_eval.py\", line 46, in update\n","    img_ids, eval_imgs = evaluate(coco_eval)\n","  File \"/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/tool/tv_reference/coco_eval.py\", line 339, in evaluate\n","    for imgId in p.imgIds\n","  File \"/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/tool/tv_reference/coco_eval.py\", line 340, in <dictcomp>\n","    for catId in catIds}\n","  File \"/usr/local/lib/python3.7/dist-packages/pycocotools/cocoeval.py\", line 189, in computeIoU\n","    ious = maskUtils.iou(d,g,iscrowd)\n","  File \"pycocotools/_mask.pyx\", line 214, in pycocotools._mask.iou\n","  File \"pycocotools/_mask.pyx\", line 193, in pycocotools._mask.iou._preproc\n","Exception: list input can be bounding box (Nx4) or RLEs ([RLE])\n"]}],"source":["!python trainmnv2.py -g 0 -dir '/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/data/train2017/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoorL3WLTWO5"},"outputs":[],"source":["!python demo.py  554496"]},{"cell_type":"markdown","metadata":{"id":"cH9LajhCrcxR"},"source":["## Train YOLO with Darknet - Custum Dataset - Max Batch-Size = 3000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_Y9w8eBJTxp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PuVMiY8yrzQB"},"source":["## Train YOLO with Darknet - Ts Dataset max-size = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIZz5bf0xutJ"},"outputs":[],"source":["!./darknet detector train  cfg/ts_data.data cfg/yolov4_ts_train.cfg weights/darknet53.conv/ -dont_show"]},{"cell_type":"markdown","metadata":{"id":"uYwlKAtKrTYg"},"source":["## Calculating mAP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLWd-QzMnQ-6"},"outputs":[],"source":["!./darknet detector map cfg/custom_data.data cfg/yolov4_custom_train.cfg weights/yolov4_custom_train_final.weights"]},{"cell_type":"markdown","metadata":{"id":"vq9nfZ5GZ8aG"},"source":["## Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xx81_ettrOJp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1tFaXUerVqO"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","net = Net()\n","print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHqXpehaaWT1"},"outputs":[],"source":["optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gl8MaxhWIINh"},"outputs":[],"source":["# Additional information\n","EPOCH = 5\n","PATH = \"model.pt\"\n","LOSS = 0.4\n","\n","torch.save({\n","            'epoch': EPOCH,\n","            'model_state_dict': net.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': LOSS,\n","            }, PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlCEtOX3IMYv"},"outputs":[],"source":["model = Net()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","checkpoint = torch.load(PATH)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","\n","model.eval()\n","# - or -\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3cp-WsEvnWg"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lmpdrmL0ih_G"},"source":["# YOLOv4 MobileNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6zRcHGLkUQy"},"outputs":[],"source":["#%cd Yolov4_Mobilenet/\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5rWAGGviqvt"},"outputs":[],"source":["!python3 preparation/data_split.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sofWYyTmCXTb"},"outputs":[],"source":["!python3 preparation/kmeans_for_anchors.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoF8ZDvwE4iw"},"outputs":[],"source":["!python3 preparation/voc2txt_annotation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlUcUjKH9Bks"},"outputs":[],"source":["#from google.colab import  files\n","#files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzS0DGcB3sPL"},"outputs":[],"source":["!python3 trainer/Train.py"]},{"cell_type":"markdown","metadata":{"id":"PXk47PP5ab44"},"source":["# YOLO Inference time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgnsHmjEaou1"},"outputs":[],"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","import time\n","\n","\n","np.random.seed(42)\n","classes = open('/content/gdrive/MyDrive/Uni/MA/darknet/data/coco.names').read().strip().split('\\n')\n","colours = np.random.randint(0, 255, size = (len(classes), 3), dtype = 'uint8')# print(classes)\n","then = time.perf_counter()\n","\n","def getinferencetime(func):\n","    def wrapper(*arg, **kwargs):\n","      then = time.perf_counter()\n","      outputs = func( *arg, **kwargs)\n","      now = time.perf_counter()\n","      print(f\"Taken Time: {now-then}\")\n","      return outputs\n","    return wrapper\n","\n","cfg = r\"/content/gdrive/MyDrive/Uni/OD_Training/OD_Training_Darknet/darknet/cfg/yolov4.cfg\"\n","weights = r\"/content/gdrive/MyDrive/Uni/OD_Training/OD_Training_Darknet/darknet/weights/yolov4.weights\"\n","\n","#Load YOLO Model\n","\n","@getinferencetime\n","\n","def load_model(cfg = None, weights = None):\n","    net = cv2.dnn.readNetFromDarknet(cfg, weights)\n","    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n","    layer_names = net.getLayerNames()\n","    return layer_names[net.getUnconnectedOutLayers()[-1] - 1], net\n","\n","ln, yolov4 = load_model(cfg = cfg, weights = weights)\n","\n","def read_image(image_path):\n","    img = cv2.imread(image_path)\n","    img = cv2.resize(img, (512, 512))\n","    return img\n","\n","def process_image(image_path):\n","    img = cv2.imread(image_path)# Opencv read image in BGR format\n","    img2blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (512, 512), swapRB = True, crop = False)\n","    return img2blob\n","\n","@getinferencetime\n","def yolov4_inference(blob):\n","    yolov4.setInput(blob)\n","    outputs = yolov4.forward(ln)\n","    return outputs\n","\n","@getinferencetime\n","def detect_object(outputs, img):\n","    h, w = img.shape[: 2]\n","    for detection in outputs:\n","        scores = detection[5: ]\n","        classID = np.argmax(scores)\n","        confidence = scores[classID]\n","        if confidence > 0.5:\n","            box = detection[: 4] * np.array([w, h, w, h])\n","            (centerX, centerY, width, height) = box.astype(\"int\")\n","            x = int(centerX - (width / 2))\n","            y = int(centerY - (height / 2))\n","            box = [x, y, int(width), int(height)]\n","            boxes.append(box)\n","            confidences.append(float(confidence))\n","            classIDs.append(classID)\n","    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","    if len(indices) > 0:\n","      for i in indices.flatten():\n","        (x, y) = (boxes[i][0], boxes[i][1])\n","        (w, h) = (boxes[i][2], boxes[i][3])\n","        color = [int(c) for c in colors[classIDs[i]]]\n","        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n","        text = \"{}: {:.4f}\".format(classes[classIDs[i]], confidences[i])\n","        cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n","    return img\n","\n","def renderimg(img):\n","    cv2_imshow(img)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","boxes = []\n","confidences = []\n","classIDs = []\n","\n","path = r\"/content/gdrive/MyDrive/Uni/MA/pytorch-YOLOv4/data/giraffe.jpg\"\n","img = read_image(path)\n","\n","outputs = yolov4_inference(\n","    process_image(path)\n",")\n","\n","renderimg(\n","    detect_object(\n","        outputs,\n","        img\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"oDXbhOgjdc-k"},"source":["# Knowledge Distillation - ResDistiller"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Scq5QNKeoiyo"},"outputs":[],"source":["import tensorboardX \n","print(tensorboardX.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0U7NQKepqAFo"},"outputs":[],"source":["!sudo easy_install pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfVoPdBjqBiI"},"outputs":[],"source":["!python2 -m pip install --user --upgrade pip\n","!python3 -m pip install --user --upgrade pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYvrnv20qCe7"},"outputs":[],"source":["!pip install numpy --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2nrhLGxtz3i"},"outputs":[],"source":["import numpy\n","numpy.__version__\n","#!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vFy004L69pf"},"outputs":[],"source":["!pip install numpy --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brf9oZjWt6gD"},"outputs":[],"source":["print(numpy.__path__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38LrgtADhOFY"},"outputs":[],"source":["!pip install --upgrade numpy==1.22.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-njyWC-fGFf"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"yokr4qK6bPT2"},"source":["# ODKD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQ_5UjvhbXB1"},"outputs":[],"source":["%cd Object-Detection-Knowledge-Distillation/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfUdSC6vbVBs"},"outputs":[],"source":["!python setup.py install --user"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tu6X4lCqbzvh"},"outputs":[],"source":["!python -m torch.distributed.launch --nproc_per_node=2 `which odkd-train` training_config.yml"]},{"cell_type":"markdown","metadata":{"id":"yYaANxYDCefN"},"source":["# Train YOLO With Darknet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpZJfNy54eI3"},"outputs":[],"source":["# clone darknet rep\n","%cd YOLOv4-Darknet/ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"binZHQdV3QEI"},"outputs":[],"source":["# Download COCO\n","#!wget http://images.cocodataset.org/zips/train2017.zip\n","#!wget http://images.cocodataset.org/zips/test2017.zip\n","#!wget http://images.cocodataset.org/zips/val2017.zip\n","#!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ts6Q0I3vYRwJ"},"outputs":[],"source":["# change makefile to make sure GPU and OPENCV enabled\n","%cd darknet\n","!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n","!sed -i 's/GPU=0/GPU=1/' Makefile\n","!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n","!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-O82fByYyGi"},"outputs":[],"source":["# verify CUDA on Virtual Machine\n","!/usr/local/cuda/bin/nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMIfKUIxCdbs"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNRL_jDwY7TK"},"outputs":[],"source":["# make darknet (builds the darknet to make executable file to run or train object detectors)\n","!make"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ga8zVMZY8VD"},"outputs":[],"source":["!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlOpF6DLZXbI"},"outputs":[],"source":["#object class for useful functions\n","def imShow(path):\n","  import cv2\n","  import matplotlib.pyplot as plt\n","  %matplotlib inline\n","\n","  image = cv2.imread(path)\n","  height, width = image.shape[:2]\n","  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n","\n","  fig = plt.gcf()\n","  fig.set_size_inches(18, 10)\n","  plt.axis(\"off\")\n","  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n","  plt.show()\n","\n","# use this to upload files\n","def upload():\n","  from google.colab import files\n","  uploaded = files.upload() \n","  for name, data in uploaded.items():\n","    with open(name, 'wb') as f:\n","      f.write(data)\n","      print ('saved file', name)\n","\n","# use this to download a file  \n","def download(path):\n","  from google.colab import files\n","  files.download(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j40fRtfGZY-R"},"outputs":[],"source":["!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbFpK9sMZi4C"},"outputs":[],"source":["imShow('predictions.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXgsa0R2Zmyc"},"outputs":[],"source":["#Upload new image from the computer using defined Upload object\n","%cd ..\n","upload()\n","%cd darknet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KLP3sNXZru4"},"outputs":[],"source":["# run darknet with YOLOv4 on your personal image! (note yours will not be called highway.jpg so change the name)\n","!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights ../Busyroad.jpg\n","imShow('predictions.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBQrayCDZswi"},"outputs":[],"source":["## DOWNLOAD to LOCAL MACHINE \n","download('predictions.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTyqnOQTZwCZ"},"outputs":[],"source":["person_ind = [i for i, cls in enumerate(coco.data) if cls == 'person'][0]"]},{"cell_type":"markdown","metadata":{"id":"bxo4ZFi9cdQr"},"source":["# Coversion coco to VOC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukaVDaSBc8R5"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFxUWpj8NXOU"},"outputs":[],"source":["!python utils/coco.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rpl7ePoGMRoT"},"outputs":[],"source":["import sys\n","\n","sys.path.append(\"..\")\n","import xml.etree.ElementTree as ET\n","#import config.yolov4_config.py as cfg\n","print(ET)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgQnLQxXmwCb"},"outputs":[],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"xYOShlwZNxLW"},"source":[]},{"cell_type":"markdown","metadata":{"id":"tZhPTjAMWDDG"},"source":["# Scaled YOLOV4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBZKSkM8nqGe"},"outputs":[],"source":["\n","cd ScaledYOLOv4/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmqN4lLbD6e_"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMAT4bnHLtmE"},"outputs":[],"source":["!python test.py --img 896 --conf 0.001 --batch 8 --device 0 --data coco.yaml --weights weights/yolov4-p5.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_SiCxqg44JS"},"outputs":[],"source":["!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tarer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oav43xRVIEO"},"outputs":[],"source":["message = 'Learn Python - '\n","\n","# remove leading and trailing whitespaces\n","print('Message:', message.strip())"]},{"cell_type":"markdown","metadata":{"id":"feR2l31wcGab"},"source":["# YOLOV4 MobileNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbmnGvC9cR1q"},"outputs":[],"source":["%cd Yolov4_Mobilenet/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5L9jnAXcO8q"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-JOScclcpx6"},"outputs":[],"source":["!python3 preparation/data_split.py"]},{"cell_type":"markdown","metadata":{"id":"V1ebljoX7QgE"},"source":["# MobileNet-YOLO Pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2-DpRbfLx1t"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItGv1pIl77V3"},"outputs":[],"source":["%cd Mobilenet-YOLO-Pytorch/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaX45fqP9dMq"},"outputs":[],"source":["!sh scripts/create.sh "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcmnjg8pweOc"},"outputs":[],"source":["!python3 folder2lmdb.py -d data/bdd100k.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzMZig3S7e7H"},"outputs":[],"source":["!pip install --upgrade --force-reinstall progress"]},{"cell_type":"markdown","metadata":{"id":"bkAUVyoUWRUH"},"source":["# Start Tranning "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4Fs5F87sSrd"},"outputs":[],"source":["!python3 train_distill.py --num-gpus 1 --resume --config-file configs/Distillation-ICD/retinanet_R_50_R101_icd_FPN_1x.yaml OUTPUT_DIR output/icd_retinanet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArZ886V0tEH3"},"outputs":[],"source":["!pip install PyYAML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdAAZCiFvNaD"},"outputs":[],"source":["%%time\n","# deps installation\n","try:\n","  import detectron2\n","except ImportError:\n","  !git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n","  !pip install -e detectron2_repo\n","  print('Stopping RUNTIME! Please run again.')\n","  import os\n","  os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdUyKTGv6gHa"},"outputs":[],"source":["import torch\n","# Model\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom\n","\n","# Images\n","img = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n","\n","# Inference\n","results = model(img)\n","\n","# Results\n","print('Model-Result:', results)\n","#results.print()  # or .show(), .save(), .crop(), .pandas(), etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erQVO7413ybm"},"outputs":[],"source":["#!pwd\n","%cd /content/drive/MyDrive/Uni/MA/Teacher-Student-Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvH_8v2r4ZZe"},"outputs":[],"source":["%cd yolov5\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDckm_MtrA_6"},"outputs":[],"source":["! python train.py --data coco.yaml --cfg yolov5s.yaml --weights '' --batch-size 64 #--teacher_weight yolov5s.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAxIN-aoV5Ad"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1JLW9Xn7XF5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["oDXbhOgjdc-k","yokr4qK6bPT2","yYaANxYDCefN","bxo4ZFi9cdQr","feR2l31wcGab","V1ebljoX7QgE","bkAUVyoUWRUH"],"machine_shape":"hm","provenance":[],"mount_file_id":"1dR8dA6A6ugqmv7qm17wsJDa7KrJNa6uF","authorship_tag":"ABX9TyOpNRy2QoLY86KgogMKglpq"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}